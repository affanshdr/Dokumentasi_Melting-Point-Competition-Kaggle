{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost MAE: 34.6205\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# 1. Import Library\n",
    "# ==========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ==========================\n",
    "# 2. Load Dataset\n",
    "# ==========================\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Misalnya target harga rumah\n",
    "y = df[\"Tm\"]   # atau ganti dengan \"Tm\" di dataset kamu\n",
    "X = df.drop(columns=[\"Tm\"])\n",
    "\n",
    "# ==========================\n",
    "# 3. Train / Valid Split\n",
    "# ==========================\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 4. Preprocessing\n",
    "# ==========================\n",
    "# Pisahkan kolom numerik & kategorikal\n",
    "num_cols = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Preprocessor: numerik → langsung, kategorikal → OneHot\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 5. Buat Pipeline dengan XGBoost (FIXED)\n",
    "# ==========================\n",
    "model = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"xgb\", XGBRegressor(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        enable_categorical=True  # ADD THIS LINE\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ==========================\n",
    "# 6. Training (FIXED - remove eval_set from fit method)\n",
    "# ==========================\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ==========================\n",
    "# 7. Evaluasi\n",
    "# ==========================\n",
    "y_pred = model.predict(X_valid)\n",
    "mae = mean_absolute_error(y_valid, y_pred)\n",
    "print(f\"XGBoost MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:85.71075\n",
      "[100]\tvalidation_0-rmse:58.57734\n",
      "[200]\tvalidation_0-rmse:55.79374\n",
      "[300]\tvalidation_0-rmse:54.08731\n",
      "[400]\tvalidation_0-rmse:53.36911\n",
      "[500]\tvalidation_0-rmse:52.92361\n",
      "[600]\tvalidation_0-rmse:52.76553\n",
      "[700]\tvalidation_0-rmse:52.47890\n",
      "[800]\tvalidation_0-rmse:52.24709\n",
      "[900]\tvalidation_0-rmse:52.12436\n",
      "[999]\tvalidation_0-rmse:51.99552\n",
      "XGBoost MAE: 34.8713\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# 1. Import Library\n",
    "# ==========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ==========================\n",
    "# 2. Load Dataset\n",
    "# ==========================\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Misalnya target harga rumah\n",
    "y = df[\"Tm\"]\n",
    "X = df.drop(columns=[\"Tm\"])\n",
    "\n",
    "# ==========================\n",
    "# 3. Train / Valid Split\n",
    "# ==========================\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 4. Manual Preprocessing\n",
    "# ==========================\n",
    "# Pisahkan kolom numerik & kategorikal\n",
    "num_cols = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# OneHot encoding untuk kategorikal\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "X_train_cat_encoded = encoder.fit_transform(X_train[cat_cols])\n",
    "X_valid_cat_encoded = encoder.transform(X_valid[cat_cols])\n",
    "\n",
    "# Gabungkan dengan numerik\n",
    "X_train_processed = np.hstack([X_train[num_cols].values, X_train_cat_encoded])\n",
    "X_valid_processed = np.hstack([X_valid[num_cols].values, X_valid_cat_encoded])\n",
    "\n",
    "# ==========================\n",
    "# 5. Training dengan XGBoost\n",
    "# ==========================\n",
    "model = XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train_processed, y_train,\n",
    "    eval_set=[(X_valid_processed, y_valid)],\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 6. Evaluasi\n",
    "# ==========================\n",
    "y_pred = model.predict(X_valid_processed)\n",
    "mae = mean_absolute_error(y_valid, y_pred)\n",
    "print(f\"XGBoost MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:85.56426\n",
      "[100]\tvalidation_0-rmse:57.62781\n",
      "[200]\tvalidation_0-rmse:54.89076\n",
      "[300]\tvalidation_0-rmse:53.62912\n",
      "[400]\tvalidation_0-rmse:52.92441\n",
      "[500]\tvalidation_0-rmse:52.50955\n",
      "[600]\tvalidation_0-rmse:52.24292\n",
      "[700]\tvalidation_0-rmse:51.99659\n",
      "[800]\tvalidation_0-rmse:51.78125\n",
      "[900]\tvalidation_0-rmse:51.69864\n",
      "[999]\tvalidation_0-rmse:51.58549\n",
      "XGBoost MAE: 34.6302\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# 1. Import Library\n",
    "# ==========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ==========================\n",
    "# 2. Load Dataset\n",
    "# ==========================\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "y = df[\"Tm\"]\n",
    "X = df.drop(columns=[\"Tm\"])\n",
    "\n",
    "# ==========================\n",
    "# 3. Train / Valid Split\n",
    "# ==========================\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 4. Preprocessing\n",
    "# ==========================\n",
    "num_cols = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 5. Preprocess data manually for eval_set\n",
    "# ==========================\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_valid_processed = preprocessor.transform(X_valid)\n",
    "\n",
    "# ==========================\n",
    "# 6. Training dengan early stopping\n",
    "# ==========================\n",
    "model = XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train_processed, y_train,\n",
    "    eval_set=[(X_valid_processed, y_valid)],\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 7. Evaluasi\n",
    "# ==========================\n",
    "y_pred = model.predict(X_valid_processed)\n",
    "mae = mean_absolute_error(y_valid, y_pred)\n",
    "print(f\"XGBoost MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:SMILES: object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\xgboost\\data.py:407\u001b[39m, in \u001b[36mpandas_feature_info\u001b[39m\u001b[34m(data, meta, feature_names, feature_types, enable_categorical)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     new_feature_types.append(\u001b[43m_pandas_dtype_mapper\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: 'object'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m df_test = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33mtest.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Prediksi\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m y_pred = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Buat submission file\u001b[39;00m\n\u001b[32m      7\u001b[39m submission = pd.DataFrame({\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: df_test[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTm\u001b[39m\u001b[33m\"\u001b[39m: y_pred\n\u001b[32m     10\u001b[39m })\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\xgboost\\sklearn.py:1327\u001b[39m, in \u001b[36mXGBModel.predict\u001b[39m\u001b[34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[39m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._can_use_inplace_predict():\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1327\u001b[39m         predts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmargin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_alike(predts):\n\u001b[32m   1336\u001b[39m             cp = import_cupy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\xgboost\\core.py:2665\u001b[39m, in \u001b[36mBooster.inplace_predict\u001b[39m\u001b[34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[39m\n\u001b[32m   2663\u001b[39m     data = pd.DataFrame(data)\n\u001b[32m   2664\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_df(data):\n\u001b[32m-> \u001b[39m\u001b[32m2665\u001b[39m     data, fns, _ = \u001b[43m_transform_pandas_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2666\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m validate_features:\n\u001b[32m   2667\u001b[39m         \u001b[38;5;28mself\u001b[39m._validate_features(fns)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\xgboost\\data.py:640\u001b[39m, in \u001b[36m_transform_pandas_df\u001b[39m\u001b[34m(data, enable_categorical, feature_names, feature_types, meta)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data.columns) > \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m meta \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _matrix_meta:\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataFrame for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeta\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cannot have multiple columns\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m feature_names, feature_types = \u001b[43mpandas_feature_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    644\u001b[39m arrays = pandas_transform_data(data)\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m PandasTransformed(arrays), feature_names, feature_types\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\xgboost\\data.py:409\u001b[39m, in \u001b[36mpandas_feature_info\u001b[39m\u001b[34m(data, meta, feature_names, feature_types, enable_categorical)\u001b[39m\n\u001b[32m    407\u001b[39m             new_feature_types.append(_pandas_dtype_mapper[dtype.name])\n\u001b[32m    408\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m             \u001b[43m_invalid_dataframe_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m feature_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m meta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    412\u001b[39m     feature_types = new_feature_types\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\xgboost\\data.py:372\u001b[39m, in \u001b[36m_invalid_dataframe_dtype\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    370\u001b[39m type_err = \u001b[33m\"\u001b[39m\u001b[33mDataFrame.dtypes for data must be int, float, bool or category.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    371\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_err\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_ENABLE_CAT_ERR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:SMILES: object"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Prediksi\n",
    "y_pred = model.predict(df_test)\n",
    "\n",
    "# Buat submission file\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": df_test[\"id\"],\n",
    "    \"Tm\": y_pred\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"File submission.csv berhasil dibuat!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost MAE: 34.6205\n",
      "File submission.csv berhasil dibuat!\n",
      "Shape submission: (666, 2)\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# 1. Import Library\n",
    "# ==========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ==========================\n",
    "# 2. Load Dataset\n",
    "# ==========================\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "y = df[\"Tm\"]\n",
    "X = df.drop(columns=[\"Tm\"])\n",
    "\n",
    "# ==========================\n",
    "# 3. Train / Valid Split\n",
    "# ==========================\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 4. Preprocessing\n",
    "# ==========================\n",
    "num_cols = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 5. Buat Pipeline dengan XGBoost\n",
    "# ==========================\n",
    "model = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"xgb\", XGBRegressor(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        enable_categorical=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ==========================\n",
    "# 6. Training\n",
    "# ==========================\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ==========================\n",
    "# 7. Evaluasi\n",
    "# ==========================\n",
    "y_pred_valid = model.predict(X_valid)\n",
    "mae = mean_absolute_error(y_valid, y_pred_valid)\n",
    "print(f\"XGBoost MAE: {mae:.4f}\")\n",
    "\n",
    "# ==========================\n",
    "# 8. PREDIKSI DATA TEST\n",
    "# ==========================\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "y_pred_test = model.predict(df_test)\n",
    "\n",
    "# Buat submission file\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": df_test[\"id\"],\n",
    "    \"Tm\": y_pred_test\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"File submission.csv berhasil dibuat!\")\n",
    "print(f\"Shape submission: {submission.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data shape: (2662, 427)\n",
      "Target stats - Min: 53.54, Max: 897.15, Mean: 278.26\n",
      "Engineering features...\n",
      "Numeric columns: 425\n",
      "Categorical columns: 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# 3.3 Statistical features untuk categorical columns\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cat_cols:\n\u001b[32m     58\u001b[39m     \u001b[38;5;66;03m# Group statistical features\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     group_stats = \u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnum_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstd\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.fillna(\u001b[32m0\u001b[39m)\n\u001b[32m     60\u001b[39m     group_stats.columns = [\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstat\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m num \u001b[38;5;129;01min\u001b[39;00m num_cols \u001b[38;5;28;01mfor\u001b[39;00m stat \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mstd\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     61\u001b[39m     X = pd.concat([X, group_stats], axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\groupby\\generic.py:1815\u001b[39m, in \u001b[36mDataFrameGroupBy.transform\u001b[39m\u001b[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m   1812\u001b[39m \u001b[38;5;129m@Substitution\u001b[39m(klass=\u001b[33m\"\u001b[39m\u001b[33mDataFrame\u001b[39m\u001b[33m\"\u001b[39m, example=__examples_dataframe_doc)\n\u001b[32m   1813\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(_transform_template)\n\u001b[32m   1814\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, *args, engine=\u001b[38;5;28;01mNone\u001b[39;00m, engine_kwargs=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m1815\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1816\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1817\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\groupby\\groupby.py:2022\u001b[39m, in \u001b[36mGroupBy._transform\u001b[39m\u001b[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m   2018\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   2019\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, *args, engine=\u001b[38;5;28;01mNone\u001b[39;00m, engine_kwargs=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m   2020\u001b[39m     \u001b[38;5;66;03m# optimized transforms\u001b[39;00m\n\u001b[32m   2021\u001b[39m     orig_func = func\n\u001b[32m-> \u001b[39m\u001b[32m2022\u001b[39m     func = \u001b[43mcom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_cython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m func\n\u001b[32m   2023\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m orig_func != func:\n\u001b[32m   2024\u001b[39m         warn_alias_replacement(\u001b[38;5;28mself\u001b[39m, orig_func, func)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\common.py:630\u001b[39m, in \u001b[36mget_cython_func\u001b[39m\u001b[34m(arg)\u001b[39m\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_cython_func\u001b[39m(arg: Callable) -> \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    627\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    628\u001b[39m \u001b[33;03m    if we define an internal function for this argument, return it\u001b[39;00m\n\u001b[32m    629\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_cython_table\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# 1. IMPORT LIBRARIES\n",
    "# ==========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, QuantileTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from scipy import stats\n",
    "from scipy.stats.mstats import winsorize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==========================\n",
    "# 2. LOAD & EXPLORE DATA\n",
    "# ==========================\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "\n",
    "# Pastikan target exists\n",
    "if 'Tm' not in df.columns:\n",
    "    raise ValueError(\"Column 'Tm' not found in dataset!\")\n",
    "\n",
    "y = df[\"Tm\"]\n",
    "X = df.drop(columns=[\"Tm\"])\n",
    "\n",
    "print(f\"Target stats - Min: {y.min():.2f}, Max: {y.max():.2f}, Mean: {y.mean():.2f}\")\n",
    "\n",
    "# ==========================\n",
    "# 3. ADVANCED FEATURE ENGINEERING\n",
    "# ==========================\n",
    "print(\"Engineering features...\")\n",
    "\n",
    "# 3.1 Basic feature types\n",
    "num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "print(f\"Numeric columns: {len(num_cols)}\")\n",
    "print(f\"Categorical columns: {len(cat_cols)}\")\n",
    "\n",
    "# 3.2 Create synthetic features (jika ada numeric columns)\n",
    "if num_cols:\n",
    "    # Polynomial features sederhana\n",
    "    for i, col1 in enumerate(num_cols):\n",
    "        for col2 in num_cols[i+1:]:\n",
    "            X[f'{col1}_x_{col2}'] = X[col1] * X[col2]\n",
    "            X[f'{col1}_div_{col2}'] = X[col1] / (X[col2] + 1e-8)  # avoid division by zero\n",
    "\n",
    "# 3.3 Statistical features untuk categorical columns\n",
    "for col in cat_cols:\n",
    "    # Group statistical features\n",
    "    group_stats = X.groupby(col)[num_cols].transform(['mean', 'std']).fillna(0)\n",
    "    group_stats.columns = [f'{col}_{stat}_{num}' for num in num_cols for stat in ['mean', 'std']]\n",
    "    X = pd.concat([X, group_stats], axis=1)\n",
    "\n",
    "# Update columns setelah feature engineering\n",
    "num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "print(f\"After engineering - Numeric: {len(num_cols)}, Categorical: {len(cat_cols)}\")\n",
    "\n",
    "# ==========================\n",
    "# 4. OUTLIER HANDLING\n",
    "# ==========================\n",
    "print(\"Handling outliers...\")\n",
    "\n",
    "# 4.1 Detect outliers menggunakan IQR method\n",
    "outlier_mask = np.zeros(len(X), dtype=bool)\n",
    "\n",
    "for col in num_cols:\n",
    "    if X[col].nunique() > 5:  # Hanya untuk continuous variables\n",
    "        Q1 = X[col].quantile(0.25)\n",
    "        Q3 = X[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        col_outliers = (X[col] < lower_bound) | (X[col] > upper_bound)\n",
    "        outlier_mask = outlier_mask | col_outliers\n",
    "\n",
    "print(f\"Found {outlier_mask.sum()} outliers ({outlier_mask.sum()/len(X)*100:.1f}%)\")\n",
    "\n",
    "# 4.2 Remove outliers\n",
    "X_clean = X[~outlier_mask].copy()\n",
    "y_clean = y[~outlier_mask].copy()\n",
    "\n",
    "print(f\"Data after outlier removal: {X_clean.shape}\")\n",
    "\n",
    "# ==========================\n",
    "# 5. TARGET TRANSFORMATION\n",
    "# ==========================\n",
    "print(\"Transforming target...\")\n",
    "\n",
    "# Cek skewness target\n",
    "skewness = stats.skew(y_clean)\n",
    "print(f\"Target skewness: {skewness:.3f}\")\n",
    "\n",
    "if abs(skewness) > 1:  # Jika highly skewed\n",
    "    y_transformed = np.log1p(y_clean)\n",
    "    use_log_transform = True\n",
    "    print(\"Applied log transformation to target\")\n",
    "else:\n",
    "    y_transformed = y_clean.copy()\n",
    "    use_log_transform = False\n",
    "    print(\"No transformation needed for target\")\n",
    "\n",
    "# ==========================\n",
    "# 6. TRAIN-VALID SPLIT\n",
    "# ==========================\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_clean, y_transformed, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Valid shape: {X_valid.shape}\")\n",
    "\n",
    "# ==========================\n",
    "# 7. PREPROCESSING PIPELINE\n",
    "# ==========================\n",
    "print(\"Building preprocessing pipeline...\")\n",
    "\n",
    "# Preprocessor untuk numeric dan categorical\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', RobustScaler()),  # Robust terhadap outliers\n",
    "    ('quantile', QuantileTransformer(output_distribution='normal'))  # Normal distribution\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_cols),\n",
    "        ('cat', categorical_transformer, cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 8. ADVANCED MODEL ENSEMBLE\n",
    "# ==========================\n",
    "print(\"Building ensemble model...\")\n",
    "\n",
    "# Base models\n",
    "base_models = [\n",
    "    ('xgb1', XGBRegressor(\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )),\n",
    "    ('xgb2', XGBRegressor(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=4,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.7,\n",
    "        random_state=123\n",
    "    )),\n",
    "    ('rf', RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "    ('ridge', Ridge(alpha=1.0))\n",
    "]\n",
    "\n",
    "# Final meta-model\n",
    "meta_model = XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Stacking ensemble\n",
    "ensemble_model = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,\n",
    "    passthrough=True  # Gunakan original features juga\n",
    ")\n",
    "\n",
    "# Full pipeline dengan preprocessing\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('ensemble', ensemble_model)\n",
    "])\n",
    "\n",
    "# ==========================\n",
    "# 9. TRAINING WITH CROSS-VALIDATION\n",
    "# ==========================\n",
    "print(\"Training model with cross-validation...\")\n",
    "\n",
    "# Cross-validation untuk evaluasi\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(full_pipeline, X_train, y_train, \n",
    "                           cv=kf, scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(f\"Cross-validation MAE: {-cv_scores.mean():.4f} (+/- {-cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# Train final model pada seluruh training data\n",
    "print(\"Training final model...\")\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# ==========================\n",
    "# 10. VALIDATION EVALUATION\n",
    "# ==========================\n",
    "print(\"Evaluating on validation set...\")\n",
    "\n",
    "# Predict on validation set\n",
    "y_valid_pred = full_pipeline.predict(X_valid)\n",
    "\n",
    "# Reverse transformation jika digunakan\n",
    "if use_log_transform:\n",
    "    y_valid_pred = np.expm1(y_valid_pred)\n",
    "    y_valid_actual = np.expm1(y_valid)\n",
    "else:\n",
    "    y_valid_actual = y_valid\n",
    "\n",
    "# Calculate MAE\n",
    "val_mae = mean_absolute_error(y_valid_actual, y_valid_pred)\n",
    "print(f\"Validation MAE: {val_mae:.4f}\")\n",
    "\n",
    "'''\n",
    "# ==========================\n",
    "# 11. PREDICT ON TEST DATA\n",
    "# ==========================\n",
    "print(\"Making predictions on test data...\")\n",
    "\n",
    "# Load test data\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Apply same feature engineering ke test data\n",
    "if num_cols:  # Hanya jika ada numeric columns original\n",
    "    original_num_cols = [col for col in num_cols if col in df_test.columns and \n",
    "                        not any(x in col for x in ['_x_', '_div_', '_mean_', '_std_'])]\n",
    "    \n",
    "    for i, col1 in enumerate(original_num_cols):\n",
    "        for col2 in original_num_cols[i+1:]:\n",
    "            if col1 in df_test.columns and col2 in df_test.columns:\n",
    "                df_test[f'{col1}_x_{col2}'] = df_test[col1] * df_test[col2]\n",
    "                df_test[f'{col1}_div_{col2}'] = df_test[col1] / (df_test[col2] + 1e-8)\n",
    "\n",
    "# Statistical features untuk test data\n",
    "for col in cat_cols:\n",
    "    if col in df_test.columns:\n",
    "        # Gunakan stats dari training data untuk consistency\n",
    "        for num_col in original_num_cols:\n",
    "            if f'{col}_mean_{num_col}' in X_train.columns:\n",
    "                # Untuk test data, kita perlu mapping yang konsisten\n",
    "                # Di sini kita sederhanakan dengan mean global\n",
    "                df_test[f'{col}_mean_{num_col}'] = X_train[num_col].mean()\n",
    "                df_test[f'{col}_std_{num_col}'] = X_train[num_col].std()\n",
    "\n",
    "# Predict\n",
    "test_pred = full_pipeline.predict(df_test)\n",
    "\n",
    "# Reverse transformation\n",
    "if use_log_transform:\n",
    "    test_pred = np.expm1(test_pred)\n",
    "\n",
    "# ==========================\n",
    "# 12. CREATE SUBMISSION FILE\n",
    "# ==========================\n",
    "print(\"Creating submission file...\")\n",
    "\n",
    "# Buat submission\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": df_test[\"id\"] if 'id' in df_test.columns else df_test.index,\n",
    "    \"Tm\": test_pred\n",
    "})\n",
    "\n",
    "# Post-processing: clip predictions ke range yang reasonable\n",
    "if 'Tm' in df.columns:\n",
    "    tm_min, tm_max = df['Tm'].min(), df['Tm'].max()\n",
    "    submission['Tm'] = submission['Tm'].clip(tm_min * 0.9, tm_max * 1.1)\n",
    "\n",
    "submission.to_csv(\"submission_advanced.csv\", index=False)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"FINAL RESULTS:\")\n",
    "print(f\"Cross-validation MAE: {-cv_scores.mean():.4f}\")\n",
    "print(f\"Validation MAE: {val_mae:.4f}\")\n",
    "print(f\"Test predictions shape: {submission.shape}\")\n",
    "print(\"File saved: submission_advanced.csv\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ==========================\n",
    "# 13. FEATURE IMPORTANCE (Optional)\n",
    "# ==========================\n",
    "# Untuk XGBoost feature importance\n",
    "try:\n",
    "    # Get feature names setelah preprocessing\n",
    "    preprocessor.fit(X_train)\n",
    "    feature_names = []\n",
    "    \n",
    "    # Numeric features\n",
    "    feature_names.extend(num_cols)\n",
    "    \n",
    "    # Categorical features (setelah one-hot)\n",
    "    if cat_cols:\n",
    "        ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "        cat_features = ohe.get_feature_names_out(cat_cols)\n",
    "        feature_names.extend(cat_features)\n",
    "    \n",
    "    # Get feature importance dari meta-model\n",
    "    meta_model = full_pipeline.named_steps['ensemble'].final_estimator_\n",
    "    \n",
    "    if hasattr(meta_model, 'feature_importances_'):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names[:len(meta_model.feature_importances_)],\n",
    "            'importance': meta_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nTop 10 Feature Importances:\")\n",
    "        print(importance_df.head(10))\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Feature importance analysis skipped: {e}\")\n",
    "\n",
    "print(\"\\nAdvanced pipeline completed successfully! 🚀\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
